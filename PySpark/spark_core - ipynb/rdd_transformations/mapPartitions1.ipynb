{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"mapPartitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#   mapPartitions - Example 1\n",
    "#########################################\n",
    "rdd_parallel = sc.parallelize(range(1, 12), 3)  #(1,2,3), (4,5,6,7), (8,9,10,11)\n",
    "\n",
    "def total(iterator): \n",
    "    yield sum(iterator)\n",
    "\n",
    "rdd_mp = rdd_parallel.mapPartitions(total)\n",
    "print(rdd_mp.collect())\n",
    "\n",
    "\n",
    "rdd = sc.parallelize(range(1,12),3)\n",
    "\n",
    "def total(iterator):\n",
    "    yield sum(iterator)\n",
    "    \n",
    "rdd1 = rdd.mapPartitions(total)\n",
    "print(rdd1.collect())    \n",
    "\n",
    "def show(iterator):\n",
    "    yield list(iterator)\n",
    "\n",
    "rdd2 = rdd.mapPartitions(show)\n",
    "print(rdd2.collect())\n",
    "\n",
    "rdd3 = sc.parallelize(range(1,12))\n",
    "print(rdd3.getNumPartitions())\n",
    "\n",
    "rdd4 = rdd3.mapPartitions(total)\n",
    "print(rdd4.collect())\n",
    "\n",
    "print(\"Default no. of partitions: \", sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#   mapPartitions - Example 2\n",
    "#########################################\n",
    "def show(iterator): \n",
    "    yield list(iterator)\n",
    "\n",
    "rdd_mp2 = rdd_parallel.mapPartitions(show)\n",
    "print(rdd_mp2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "#   mapPartitions - Example 3\n",
    "##########################################\n",
    "\n",
    "rdd_parallel_2 = sc.parallelize(range(1, 12))\n",
    "\n",
    "print( rdd_parallel_2.getNumPartitions() )\n",
    "\n",
    "rdd_mp_2 = rdd_parallel_2.mapPartitions(total)\n",
    "print(rdd_mp_2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the default parallelism on this system?\n",
    "print(\"Default number of partitions: \", sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
