{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/user2/trainig_material/PySpark_Code/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"partitionBy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = range(1,30)\n",
    "\n",
    "rdd1 = sc.parallelize(nums, 2)\n",
    "print(\"rdd1 partitions: {}\".format( rdd1.getNumPartitions() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of partitions: {}\".format(rdd1.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd1.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd1.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitionBy requires data to be in (k,v) pairs\n",
    "# splitting data into 3 chunks using default hash partitioner\n",
    "rdd2 = sc.parallelize(nums) \\\n",
    "        .map(lambda el: (el, el)) \\\n",
    "        .partitionBy(3) \\\n",
    "        .persist()\n",
    "    \n",
    "print(\"Number of partitions: {}\".format(rdd2.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd2.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd2.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Custom Partitioner Example\n",
    "##################################\n",
    "transactions = [\n",
    "    {'name': 'Raju', 'amount': 100, 'city': 'Chennai'},\n",
    "    {'name': 'Mahesh', 'amount': 15, 'city': 'Hyderabad'},\n",
    "    {'name': 'Madhu', 'amount': 51, 'city': 'Hyderabad'},\n",
    "    {'name': 'Revati', 'amount': 200, 'city': 'Chennai'},\n",
    "    {'name': 'Amrita', 'amount': 75, 'city': 'Pune'},\n",
    "    {'name': 'Aditya', 'amount': 175, 'city': 'Bangalore'},\n",
    "    {'name': 'Keertana', 'amount': 105, 'city': 'Pune'},\n",
    "    {'name': 'Keertana', 'amount': 105, 'city': 'Vijayawada'}\n",
    "]\n",
    "\n",
    "def city_partitioner(city):\n",
    "    return hash(city) % 3\n",
    "\n",
    "# Validate results\n",
    "num_partitions = 3\n",
    "\n",
    "rdd3 = sc.parallelize(transactions) \\\n",
    "        .map(lambda e: (e['city'], e)) \\\n",
    "        .partitionBy(num_partitions, city_partitioner)\n",
    "    \n",
    "print(\"Number of partitions: {}\".format(rdd3.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd3.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd3.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
